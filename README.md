Certainly, I'd be happy to help you create a README file for your GitHub repository based on the information provided. Here's a draft of the README in English, divided into sections for the two projects:

# Gradient Descent Optimization Projects

This repository contains two projects focused on gradient descent optimization techniques, completed in 2023 and 2024. These projects demonstrate the application of various gradient descent methods to both synthetic and real-world datasets.

## Project 2023: Gradient Descent and BCGD Methods

### Overview
This project, completed on May 15, 2023, explores semi-supervised learning problems using Gradient Descent techniques and Block Coordinate Gradient Descent (BCGD) methods.

### Key Features
- Generation of synthetic dataset using sklearn
- Implementation of weight computation methods
- Loss function and gradient computation
- Accuracy evaluation
- Comparison of different gradient descent methods:
  - Standard Gradient Descent
  - Gradient Descent with Improved Rate
  - BCGD Random
  - BCGD Gauss-Southwell

### Datasets
- Synthetic dataset: 10,000 points divided into 2 clusters
- Real-world dataset: Surgical dataset for predicting in-hospital complications

### Results
The project compares the performance of different methods in terms of accuracy, loss, and CPU time usage for both synthetic and real-world datasets.

## Project 2024: Multi-class Logistic Regression Optimization

### Overview
This project, completed in 2024, focuses on minimizing a loss function in a multi-class logistic regression problem using Gradient Descent and Block Coordinate Gradient Descent with Gauss-Southwell rule.

### Key Features
- Implementation of Gradient Descent and BCGD with Gauss-Southwell rule
- Custom loss function and gradient computation
- Softmax function implementation
- Accuracy evaluation

### Datasets
- Synthetic dataset: 1000x1000 matrix generated from standard normal distribution
- MNIST dataset: 10,000 samples of handwritten digits

### Results
The project presents comparative results of Gradient Descent and BCGD with Gauss-Southwell rule on both synthetic and MNIST datasets, including accuracy and loss convergence.

## Repository Contents
- `Homework2023.ipynb`: Jupyter notebook for the 2023 project
- `Homework2024.ipynb`: Jupyter notebook for the 2024 project
- `Report2023.pdf`: Detailed report of the 2023 project
- `Report2024.pdf`: Detailed report of the 2024 project
- `diabetes.csv`: Dataset used in the projects
- `Surgical-deepnet.csv`: Dataset used in the projects
- `winequality-red.csv`: Dataset used in the projects

## Technologies Used
- Python
- Jupyter Notebook
- NumPy
- scikit-learn
- Matplotlib (for visualizations)

This repository showcases the progression of skills and understanding in gradient descent optimization techniques over two consecutive years, demonstrating practical applications on both synthetic and real-world datasets.
